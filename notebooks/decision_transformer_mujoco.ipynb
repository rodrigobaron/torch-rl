{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c605eef-0724-461c-b714-4532e11401cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "curl -O https://dl.google.com/dl/cloudsdk/channels/rapid/downloads/google-cloud-cli-405.0.0-linux-x86_64.tar.gz\n",
    "tar -xf google-cloud-cli-405.0.0-linux-x86_64.tar.gz\n",
    "mkdir dqn_replay\n",
    "./google-cloud-sdk/bin/gsutil -m cp -R gs://atari-replay-datasets/dqn/Breakout dqn_replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58053ef6-3cd8-4ac9-aa28-3e55b2759a83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-15T22:32:18.152681Z",
     "iopub.status.busy": "2022-10-15T22:32:18.150540Z",
     "iopub.status.idle": "2022-10-15T22:33:54.330732Z",
     "shell.execute_reply": "2022-10-15T22:33:54.329732Z",
     "shell.execute_reply.started": "2022-10-15T22:32:18.152582Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/google/dopamine.git\n",
      "  Cloning https://github.com/google/dopamine.git to /tmp/pip-req-build-x9fsdh3a\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone -q https://github.com/google/dopamine.git /tmp/pip-req-build-x9fsdh3a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Pillow>=7.0.0 in /usr/local/lib/python3.8/dist-packages (from dopamine-rl==4.0.6) (9.2.0)\n",
      "Requirement already satisfied: absl-py>=0.9.0 in /usr/local/lib/python3.8/dist-packages (from dopamine-rl==4.0.6) (1.2.0)\n",
      "Collecting flax>=0.2.0\n",
      "  Downloading flax-0.6.1-py3-none-any.whl (185 kB)\n",
      "Collecting gin-config>=0.3.0\n",
      "  Downloading gin_config-0.5.0-py3-none-any.whl (61 kB)\n",
      "Requirement already satisfied: gym<=0.25.2 in /usr/local/lib/python3.8/dist-packages (from dopamine-rl==4.0.6) (0.21.0)\n",
      "Collecting jax>=0.1.72\n",
      "  Downloading jax-0.3.23.tar.gz (1.1 MB)\n",
      "Collecting jaxlib>=0.1.51\n",
      "  Downloading jaxlib-0.3.22-cp38-cp38-manylinux2014_x86_64.whl (72.0 MB)\n",
      "Requirement already satisfied: numpy>=1.16.4 in /usr/local/lib/python3.8/dist-packages (from dopamine-rl==4.0.6) (1.23.1)\n",
      "Requirement already satisfied: opencv-python>=3.4.8.29 in /usr/local/lib/python3.8/dist-packages (from dopamine-rl==4.0.6) (4.6.0.66)\n",
      "Requirement already satisfied: pandas>=0.24.2 in /usr/local/lib/python3.8/dist-packages (from dopamine-rl==4.0.6) (1.5.0)\n",
      "Collecting pygame>=1.9.2\n",
      "  Downloading pygame-2.1.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.8 MB)\n",
      "Collecting tensorflow-probability>=0.13.0\n",
      "  Downloading tensorflow_probability-0.18.0-py2.py3-none-any.whl (6.6 MB)\n",
      "Collecting tensorflow>=2.2.0\n",
      "  Downloading tensorflow-2.10.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (578.1 MB)\n",
      "Collecting tf_slim>=1.0\n",
      "  Downloading tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n",
      "Collecting msgpack\n",
      "  Downloading msgpack-1.0.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (322 kB)\n",
      "Collecting rich>=11.1\n",
      "  Downloading rich-12.6.0-py3-none-any.whl (237 kB)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from flax>=0.2.0->dopamine-rl==4.0.6) (3.6.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.8/dist-packages (from flax>=0.2.0->dopamine-rl==4.0.6) (4.4.0)\n",
      "Collecting optax\n",
      "  Downloading optax-0.1.3-py3-none-any.whl (145 kB)\n",
      "Collecting PyYAML>=5.4.1\n",
      "  Downloading PyYAML-6.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (701 kB)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym<=0.25.2->dopamine-rl==4.0.6) (2.2.0)\n",
      "Collecting etils[epath]\n",
      "  Downloading etils-0.8.0-py3-none-any.whl (127 kB)\n",
      "Collecting opt_einsum\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.8/dist-packages (from jax>=0.1.72->dopamine-rl==4.0.6) (1.9.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.8/dist-packages (from pandas>=0.24.2->dopamine-rl==4.0.6) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas>=0.24.2->dopamine-rl==4.0.6) (2022.4)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from tensorflow-probability>=0.13.0->dopamine-rl==4.0.6) (5.1.1)\n",
      "Collecting gast>=0.3.2\n",
      "  Downloading gast-0.5.3-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/lib/python3/dist-packages (from tensorflow-probability>=0.13.0->dopamine-rl==4.0.6) (1.14.0)\n",
      "Requirement already satisfied: dm-tree in /usr/local/lib/python3.8/dist-packages (from tensorflow-probability>=0.13.0->dopamine-rl==4.0.6) (0.1.7)\n",
      "Collecting keras<2.11,>=2.10.0\n",
      "  Downloading keras-2.10.0-py2.py3-none-any.whl (1.7 MB)\n",
      "Collecting flatbuffers>=2.0\n",
      "  Downloading flatbuffers-22.9.24-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2.0->dopamine-rl==4.0.6) (21.3)\n",
      "Collecting tensorflow-estimator<2.11,>=2.10.0\n",
      "  Downloading tensorflow_estimator-2.10.0-py2.py3-none-any.whl (438 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.27.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from tensorflow>=2.2.0->dopamine-rl==4.0.6) (45.2.0)\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-14.0.6-py2.py3-none-manylinux2010_x86_64.whl (14.1 MB)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.49.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2.0->dopamine-rl==4.0.6) (3.7.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2.0->dopamine-rl==4.0.6) (1.14.1)\n",
      "Collecting protobuf<3.20,>=3.9.2\n",
      "  Downloading protobuf-3.19.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2.0->dopamine-rl==4.0.6) (2.0.1)\n",
      "Collecting tensorboard<2.11,>=2.10\n",
      "  Downloading tensorboard-2.10.1-py3-none-any.whl (5.9 MB)\n",
      "Collecting keras-preprocessing>=1.1.1\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Collecting commonmark<0.10.0,>=0.9.0\n",
      "  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.8/dist-packages (from rich>=11.1->flax>=0.2.0->dopamine-rl==4.0.6) (2.13.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->flax>=0.2.0->dopamine-rl==4.0.6) (1.0.5)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib->flax>=0.2.0->dopamine-rl==4.0.6) (4.37.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->flax>=0.2.0->dopamine-rl==4.0.6) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->flax>=0.2.0->dopamine-rl==4.0.6) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->flax>=0.2.0->dopamine-rl==4.0.6) (0.11.0)\n",
      "Collecting chex>=0.0.4\n",
      "  Downloading chex-0.1.5-py3-none-any.whl (85 kB)\n",
      "Requirement already satisfied: zipp; extra == \"epath\" in /usr/local/lib/python3.8/dist-packages (from etils[epath]->jax>=0.1.72->dopamine-rl==4.0.6) (3.9.0)\n",
      "Requirement already satisfied: importlib_resources; extra == \"epath\" in /usr/local/lib/python3.8/dist-packages (from etils[epath]->jax>=0.1.72->dopamine-rl==4.0.6) (5.10.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/lib/python3/dist-packages (from astunparse>=1.6.0->tensorflow>=2.2.0->dopamine-rl==4.0.6) (0.34.2)\n",
      "Collecting werkzeug>=1.0.1\n",
      "  Downloading Werkzeug-2.2.2-py3-none-any.whl (232 kB)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/lib/python3/dist-packages (from tensorboard<2.11,>=2.10->tensorflow>=2.2.0->dopamine-rl==4.0.6) (2.22.0)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.12.0-py2.py3-none-any.whl (169 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.4.1-py3-none-any.whl (93 kB)\n",
      "Collecting toolz>=0.9.0\n",
      "  Downloading toolz-0.12.0-py3-none-any.whl (55 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.8/dist-packages (from werkzeug>=1.0.1->tensorboard<2.11,>=2.10->tensorflow>=2.2.0->dopamine-rl==4.0.6) (2.1.1)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.2.0-py3-none-any.whl (9.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting rsa<5,>=3.1.4; python_version >= \"3.6\"\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.4; python_version < \"3.10\" in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow>=2.2.0->dopamine-rl==4.0.6) (4.13.0)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.1-py3-none-any.whl (151 kB)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Building wheels for collected packages: dopamine-rl, jax\n",
      "  Building wheel for dopamine-rl (setup.py): started\n",
      "  Building wheel for dopamine-rl (setup.py): finished with status 'done'\n",
      "  Created wheel for dopamine-rl: filename=dopamine_rl-4.0.6-py3-none-any.whl size=179537 sha256=edd13bcecb4eeca18d2d4bc397504a131f003424303f2b6f56b3a278e5fb7057\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-_nhtkx4z/wheels/fc/65/b1/6cf690ff921239f4085683fb10228974009896687cae86e60d\n",
      "  Building wheel for jax (setup.py): started\n",
      "  Building wheel for jax (setup.py): finished with status 'done'\n",
      "  Created wheel for jax: filename=jax-0.3.23-py3-none-any.whl size=1273254 sha256=d0a85eb987b802f5cfc6ae53571509c24c0127802a294bab65b3c753fc46ad26\n",
      "  Stored in directory: /root/.cache/pip/wheels/ab/c8/70/5954ac10c8c717e940cc24a18d0d84727b5ef956b60b2193ec\n",
      "Successfully built dopamine-rl jax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: tensorflow 2.10.0 has requirement gast<=0.4.0,>=0.2.1, but you'll have gast 0.5.3 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing collected packages: msgpack, commonmark, rich, etils, opt-einsum, jax, jaxlib, toolz, chex, optax, PyYAML, flax, gin-config, pygame, gast, tensorflow-probability, keras, flatbuffers, tensorflow-estimator, tensorflow-io-gcs-filesystem, libclang, astunparse, grpcio, google-pasta, protobuf, werkzeug, tensorboard-data-server, tensorboard-plugin-wit, oauthlib, requests-oauthlib, cachetools, pyasn1, pyasn1-modules, rsa, google-auth, google-auth-oauthlib, markdown, tensorboard, keras-preprocessing, tensorflow, tf-slim, dopamine-rl\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.21.7\n",
      "    Uninstalling protobuf-4.21.7:\n",
      "      Successfully uninstalled protobuf-4.21.7\n",
      "Successfully installed PyYAML-6.0 astunparse-1.6.3 cachetools-5.2.0 chex-0.1.5 commonmark-0.9.1 dopamine-rl-4.0.6 etils-0.8.0 flatbuffers-22.9.24 flax-0.6.1 gast-0.5.3 gin-config-0.5.0 google-auth-2.12.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.49.1 jax-0.3.23 jaxlib-0.3.22 keras-2.10.0 keras-preprocessing-1.1.2 libclang-14.0.6 markdown-3.4.1 msgpack-1.0.4 oauthlib-3.2.1 opt-einsum-3.3.0 optax-0.1.3 protobuf-3.19.6 pyasn1-0.4.8 pyasn1-modules-0.2.8 pygame-2.1.2 requests-oauthlib-1.3.1 rich-12.6.0 rsa-4.9 tensorboard-2.10.1 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.10.0 tensorflow-estimator-2.10.0 tensorflow-io-gcs-filesystem-0.27.0 tensorflow-probability-0.18.0 tf-slim-1.1.0 toolz-0.12.0 werkzeug-2.2.2\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "pip install git+https://github.com/google/dopamine.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50e9bafd-5299-48d4-a5b5-3e7d0f20959c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-15T22:30:19.134535Z",
     "iopub.status.busy": "2022-10-15T22:30:19.134311Z",
     "iopub.status.idle": "2022-10-15T22:30:19.761138Z",
     "shell.execute_reply": "2022-10-15T22:30:19.760434Z",
     "shell.execute_reply.started": "2022-10-15T22:30:19.134514Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import logging\n",
    "# make deterministic\n",
    "from mingpt.utils import set_seed\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import math\n",
    "from torch.utils.data import Dataset\n",
    "from mingpt.model_atari import GPT, GPTConfig\n",
    "from mingpt.trainer_atari import Trainer, TrainerConfig\n",
    "from mingpt.utils import sample\n",
    "from collections import deque\n",
    "import random\n",
    "import torch\n",
    "import pickle\n",
    "import blosc\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbf6500e-fed8-4646-b88f-cf6a1a22bfcc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-15T22:33:54.332366Z",
     "iopub.status.busy": "2022-10-15T22:33:54.332102Z",
     "iopub.status.idle": "2022-10-15T22:33:57.601590Z",
     "shell.execute_reply": "2022-10-15T22:33:57.600834Z",
     "shell.execute_reply.started": "2022-10-15T22:33:54.332344Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-15 22:33:54.415580: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-15 22:33:54.610382: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-10-15 22:33:55.252429: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.8/dist-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2022-10-15 22:33:55.252529: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.8/dist-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2022-10-15 22:33:55.252539: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# source: https://github.com/google-research/batch_rl/blob/master/batch_rl/fixed_replay/replay_memory/fixed_replay_buffer.py\n",
    "\n",
    "import collections\n",
    "from concurrent import futures\n",
    "from dopamine.replay_memory import circular_replay_buffer\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "import gin\n",
    "\n",
    "gfile = tf.gfile\n",
    "\n",
    "STORE_FILENAME_PREFIX = circular_replay_buffer.STORE_FILENAME_PREFIX\n",
    "\n",
    "class FixedReplayBuffer(object):\n",
    "  \"\"\"Object composed of a list of OutofGraphReplayBuffers.\"\"\"\n",
    "\n",
    "  def __init__(self, data_dir, replay_suffix, *args, **kwargs):  # pylint: disable=keyword-arg-before-vararg\n",
    "    \"\"\"Initialize the FixedReplayBuffer class.\n",
    "    Args:\n",
    "      data_dir: str, log Directory from which to load the replay buffer.\n",
    "      replay_suffix: int, If not None, then only load the replay buffer\n",
    "        corresponding to the specific suffix in data directory.\n",
    "      *args: Arbitrary extra arguments.\n",
    "      **kwargs: Arbitrary keyword arguments.\n",
    "    \"\"\"\n",
    "    self._args = args\n",
    "    self._kwargs = kwargs\n",
    "    self._data_dir = data_dir\n",
    "    self._loaded_buffers = False\n",
    "    self.add_count = np.array(0)\n",
    "    self._replay_suffix = replay_suffix\n",
    "    if not self._loaded_buffers:\n",
    "      if replay_suffix is not None:\n",
    "        assert replay_suffix >= 0, 'Please pass a non-negative replay suffix'\n",
    "        self.load_single_buffer(replay_suffix)\n",
    "      else:\n",
    "        self._load_replay_buffers(num_buffers=50)\n",
    "\n",
    "  def load_single_buffer(self, suffix):\n",
    "    \"\"\"Load a single replay buffer.\"\"\"\n",
    "    replay_buffer = self._load_buffer(suffix)\n",
    "    if replay_buffer is not None:\n",
    "      self._replay_buffers = [replay_buffer]\n",
    "      self.add_count = replay_buffer.add_count\n",
    "      self._num_replay_buffers = 1\n",
    "      self._loaded_buffers = True\n",
    "\n",
    "  def _load_buffer(self, suffix):\n",
    "    \"\"\"Loads a OutOfGraphReplayBuffer replay buffer.\"\"\"\n",
    "    try:\n",
    "      # pytype: disable=attribute-error\n",
    "      replay_buffer = circular_replay_buffer.OutOfGraphReplayBuffer(\n",
    "          *self._args, **self._kwargs)\n",
    "      replay_buffer.load(self._data_dir, suffix)\n",
    "      tf.logging.info('Loaded replay buffer ckpt {} from {}'.format(\n",
    "          suffix, self._data_dir))\n",
    "      # pytype: enable=attribute-error\n",
    "      return replay_buffer\n",
    "    except tf.errors.NotFoundError:\n",
    "      return None\n",
    "\n",
    "  def _load_replay_buffers(self, num_buffers=None):\n",
    "    \"\"\"Loads multiple checkpoints into a list of replay buffers.\"\"\"\n",
    "    if not self._loaded_buffers:  # pytype: disable=attribute-error\n",
    "      ckpts = gfile.ListDirectory(self._data_dir)  # pytype: disable=attribute-error\n",
    "      # Assumes that the checkpoints are saved in a format CKPT_NAME.{SUFFIX}.gz\n",
    "      ckpt_counters = collections.Counter(\n",
    "          [name.split('.')[-2] for name in ckpts])\n",
    "      # Should contain the files for add_count, action, observation, reward,\n",
    "      # terminal and invalid_range\n",
    "      ckpt_suffixes = [x for x in ckpt_counters if ckpt_counters[x] in [6, 7]]\n",
    "      if num_buffers is not None:\n",
    "        ckpt_suffixes = np.random.choice(\n",
    "            ckpt_suffixes, num_buffers, replace=False)\n",
    "      self._replay_buffers = []\n",
    "      # Load the replay buffers in parallel\n",
    "      with futures.ThreadPoolExecutor(\n",
    "          max_workers=num_buffers) as thread_pool_executor:\n",
    "        replay_futures = [thread_pool_executor.submit(\n",
    "            self._load_buffer, suffix) for suffix in ckpt_suffixes]\n",
    "      for f in replay_futures:\n",
    "        replay_buffer = f.result()\n",
    "        if replay_buffer is not None:\n",
    "          self._replay_buffers.append(replay_buffer)\n",
    "          self.add_count = max(replay_buffer.add_count, self.add_count)\n",
    "      self._num_replay_buffers = len(self._replay_buffers)\n",
    "      if self._num_replay_buffers:\n",
    "        self._loaded_buffers = True\n",
    "\n",
    "  def get_transition_elements(self):\n",
    "    return self._replay_buffers[0].get_transition_elements()\n",
    "\n",
    "  def sample_transition_batch(self, batch_size=None, indices=None):\n",
    "    buffer_index = np.random.randint(self._num_replay_buffers)\n",
    "    return self._replay_buffers[buffer_index].sample_transition_batch(\n",
    "        batch_size=batch_size, indices=indices)\n",
    "\n",
    "  def load(self, *args, **kwargs):  # pylint: disable=unused-argument\n",
    "    pass\n",
    "\n",
    "  def reload_buffer(self, num_buffers=None):\n",
    "    self._loaded_buffers = False\n",
    "    self._load_replay_buffers(num_buffers)\n",
    "\n",
    "  def save(self, *args, **kwargs):  # pylint: disable=unused-argument\n",
    "    pass\n",
    "\n",
    "  def add(self, *args, **kwargs):  # pylint: disable=unused-argument\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ad6a8a2-e8bf-4f14-a64c-cd6bbb2af698",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-15T22:35:03.433654Z",
     "iopub.status.busy": "2022-10-15T22:35:03.433361Z",
     "iopub.status.idle": "2022-10-15T22:35:03.450325Z",
     "shell.execute_reply": "2022-10-15T22:35:03.449439Z",
     "shell.execute_reply.started": "2022-10-15T22:35:03.433630Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import logging\n",
    "# make deterministic\n",
    "from mingpt.utils import set_seed\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import math\n",
    "from torch.utils.data import Dataset\n",
    "from mingpt.model_atari import GPT, GPTConfig\n",
    "from mingpt.trainer_atari import Trainer, TrainerConfig\n",
    "from mingpt.utils import sample\n",
    "from collections import deque\n",
    "import random\n",
    "import torch\n",
    "import pickle\n",
    "import blosc\n",
    "import argparse\n",
    "\n",
    "def create_dataset(num_buffers, num_steps, game, data_dir_prefix, trajectories_per_buffer):\n",
    "    # -- load data from memory (make more efficient)\n",
    "    obss = []\n",
    "    actions = []\n",
    "    returns = [0]\n",
    "    done_idxs = []\n",
    "    stepwise_returns = []\n",
    "\n",
    "    transitions_per_buffer = np.zeros(50, dtype=int)\n",
    "    num_trajectories = 0\n",
    "    while len(obss) < num_steps:\n",
    "        buffer_num = np.random.choice(np.arange(50 - num_buffers, 50), 1)[0]\n",
    "        i = transitions_per_buffer[buffer_num]\n",
    "        print('loading from buffer %d which has %d already loaded' % (buffer_num, i))\n",
    "        frb = FixedReplayBuffer(\n",
    "            data_dir=data_dir_prefix + game + '/1/replay_logs',\n",
    "            replay_suffix=buffer_num,\n",
    "            observation_shape=(84, 84),\n",
    "            stack_size=4,\n",
    "            update_horizon=1,\n",
    "            gamma=0.99,\n",
    "            observation_dtype=np.uint8,\n",
    "            batch_size=32,\n",
    "            replay_capacity=100000)\n",
    "        if frb._loaded_buffers:\n",
    "            done = False\n",
    "            curr_num_transitions = len(obss)\n",
    "            trajectories_to_load = trajectories_per_buffer\n",
    "            while not done:\n",
    "                states, ac, ret, next_states, next_action, next_reward, terminal, indices = frb.sample_transition_batch(batch_size=1, indices=[i])\n",
    "                states = states.transpose((0, 3, 1, 2))[0] # (1, 84, 84, 4) --> (4, 84, 84)\n",
    "                obss += [states]\n",
    "                actions += [ac[0]]\n",
    "                stepwise_returns += [ret[0]]\n",
    "                if terminal[0]:\n",
    "                    done_idxs += [len(obss)]\n",
    "                    returns += [0]\n",
    "                    if trajectories_to_load == 0:\n",
    "                        done = True\n",
    "                    else:\n",
    "                        trajectories_to_load -= 1\n",
    "                returns[-1] += ret[0]\n",
    "                i += 1\n",
    "                if i >= 100000:\n",
    "                    obss = obss[:curr_num_transitions]\n",
    "                    actions = actions[:curr_num_transitions]\n",
    "                    stepwise_returns = stepwise_returns[:curr_num_transitions]\n",
    "                    returns[-1] = 0\n",
    "                    i = transitions_per_buffer[buffer_num]\n",
    "                    done = True\n",
    "            num_trajectories += (trajectories_per_buffer - trajectories_to_load)\n",
    "            transitions_per_buffer[buffer_num] = i\n",
    "        print('this buffer has %d loaded transitions and there are now %d transitions total divided into %d trajectories' % (i, len(obss), num_trajectories))\n",
    "\n",
    "    actions = np.array(actions)\n",
    "    returns = np.array(returns)\n",
    "    stepwise_returns = np.array(stepwise_returns)\n",
    "    done_idxs = np.array(done_idxs)\n",
    "\n",
    "    # -- create reward-to-go dataset\n",
    "    start_index = 0\n",
    "    rtg = np.zeros_like(stepwise_returns)\n",
    "    for i in done_idxs:\n",
    "        i = int(i)\n",
    "        curr_traj_returns = stepwise_returns[start_index:i]\n",
    "        for j in range(i-1, start_index-1, -1): # start from i-1\n",
    "            rtg_j = curr_traj_returns[j-start_index:i-start_index]\n",
    "            rtg[j] = sum(rtg_j)\n",
    "        start_index = i\n",
    "    print('max rtg is %d' % max(rtg))\n",
    "\n",
    "    # -- create timestep dataset\n",
    "    start_index = 0\n",
    "    timesteps = np.zeros(len(actions)+1, dtype=int)\n",
    "    for i in done_idxs:\n",
    "        i = int(i)\n",
    "        timesteps[start_index:i+1] = np.arange(i+1 - start_index)\n",
    "        start_index = i+1\n",
    "    print('max timestep is %d' % max(timesteps))\n",
    "\n",
    "    return obss, actions, returns, done_idxs, rtg, timesteps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f23d35-40c0-4505-8d09-d8581654c262",
   "metadata": {},
   "source": [
    "## Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4d91ea7-e940-4618-880b-6259119fb014",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-15T22:37:32.100307Z",
     "iopub.status.busy": "2022-10-15T22:37:32.099371Z",
     "iopub.status.idle": "2022-10-15T22:37:32.204006Z",
     "shell.execute_reply": "2022-10-15T22:37:32.203193Z",
     "shell.execute_reply.started": "2022-10-15T22:37:32.100225Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5239bf1e71244928802a96a8f2e3786d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=1, description='seed', tooltip='seed of the experiment')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1972c8b26f584a629f531dc6bec75a55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='Breakout', description='game', tooltip='the id of the environment')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "738c32f8fd2e4b35815bbb3517628c8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=5, description='epochs', tooltip='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41828ef22f52406f9e98020384d15c23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='reward_conditioned', description='model-type', tooltip='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a123f0b6faf44258989ac93b5d64380",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=500000, description='num-steps', tooltip='the number of steps to run in each environment')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae6a50e34f3f4d909ab484a53b93a393",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=50, description='num-buffers', tooltip='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bdeddff5f0d43a68b565e3aac904e6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=128, description='batch_size', tooltip='The number of mini-batches')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c189c97484547d79e7a41195ad97cf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=10, description='trajectories_per_buffer', tooltip='Number of trajectories to sample from each oâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fd5236596b94e32bc31756ae44de6c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='./dqn_replay/', description='data_dir_prefix', tooltip='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output, Image\n",
    "\n",
    "from torch_rl.jupyter_utils import (\n",
    "    WidgetParser,\n",
    "    plot_metrics_notebook\n",
    ")\n",
    "\n",
    "parser = WidgetParser()\n",
    "parser.add_widget(widgets.IntText(\n",
    "    description='seed',\n",
    "    value=1,\n",
    "    tooltip=\"seed of the experiment\"\n",
    "))\n",
    "\n",
    "parser.add_widget(widgets.Text(\n",
    "    description=\"game\",\n",
    "    value=\"Breakout\",\n",
    "    tooltip=\"the id of the environment\"\n",
    "))\n",
    "\n",
    "parser.add_widget(widgets.IntText(\n",
    "    description='epochs',\n",
    "    value=5,\n",
    "    tooltip=\"\"\n",
    "))\n",
    "\n",
    "parser.add_widget(widgets.Text(\n",
    "    description=\"model-type\",\n",
    "    value=\"reward_conditioned\",\n",
    "    tooltip=\"\"\n",
    "))\n",
    "\n",
    "parser.add_widget(widgets.IntText(\n",
    "    description='num-steps',\n",
    "    value=500000,\n",
    "    tooltip=\"the number of steps to run in each environment\"\n",
    "))\n",
    "\n",
    "parser.add_widget(widgets.IntText(\n",
    "    description='num-buffers',\n",
    "    value=50,\n",
    "    tooltip=\"\"\n",
    "))\n",
    "\n",
    "parser.add_widget(widgets.IntText(\n",
    "    description='batch_size',\n",
    "    value=128,\n",
    "    tooltip=\"The number of mini-batches\"\n",
    "))\n",
    "\n",
    "parser.add_widget(widgets.IntText(\n",
    "    description='trajectories_per_buffer',\n",
    "    value=10,\n",
    "    tooltip=\"Number of trajectories to sample from each of the buffers\"\n",
    "))\n",
    "\n",
    "parser.add_widget(widgets.Text(\n",
    "    description=\"data_dir_prefix\",\n",
    "    value=\"./dqn_replay/\",\n",
    "    tooltip=\"\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee6e3b2a-686a-4bf9-81d7-d51f16c724ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-15T22:37:33.507443Z",
     "iopub.status.busy": "2022-10-15T22:37:33.506524Z",
     "iopub.status.idle": "2022-10-15T22:37:33.517618Z",
     "shell.execute_reply": "2022-10-15T22:37:33.515020Z",
     "shell.execute_reply.started": "2022-10-15T22:37:33.507363Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "args = parser.parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136621fc-e7d3-463b-966b-e74388221d91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "obss, actions, returns, done_idxs, rtgs, timesteps = create_dataset(args.num_buffers, args.num_steps, args.game, args.data_dir_prefix, args.trajectories_per_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a5d6ad-7c1e-4b6b-b573-4bfe1af2b56b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f25b897-4846-4a08-90d8-faea27cd0da0",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-10-15T22:30:20.511602Z",
     "iopub.status.idle": "2022-10-15T22:30:20.511968Z",
     "shell.execute_reply": "2022-10-15T22:30:20.511833Z",
     "shell.execute_reply.started": "2022-10-15T22:30:20.511819Z"
    }
   },
   "outputs": [],
   "source": [
    "set_seed(args.seed)\n",
    "\n",
    "class StateActionReturnDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, block_size, actions, done_idxs, rtgs, timesteps):        \n",
    "        self.block_size = block_size\n",
    "        self.vocab_size = max(actions) + 1\n",
    "        self.data = data\n",
    "        self.actions = actions\n",
    "        self.done_idxs = done_idxs\n",
    "        self.rtgs = rtgs\n",
    "        self.timesteps = timesteps\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        block_size = self.block_size // 3\n",
    "        done_idx = idx + block_size\n",
    "        for i in self.done_idxs:\n",
    "            if i > idx: # first done_idx greater than idx\n",
    "                done_idx = min(int(i), done_idx)\n",
    "                break\n",
    "        idx = done_idx - block_size\n",
    "        states = torch.tensor(np.array(self.data[idx:done_idx]), dtype=torch.float32).reshape(block_size, -1) # (block_size, 4*84*84)\n",
    "        states = states / 255.\n",
    "        actions = torch.tensor(self.actions[idx:done_idx], dtype=torch.long).unsqueeze(1) # (block_size, 1)\n",
    "        rtgs = torch.tensor(self.rtgs[idx:done_idx], dtype=torch.float32).unsqueeze(1)\n",
    "        timesteps = torch.tensor(self.timesteps[idx:idx+1], dtype=torch.int64).unsqueeze(1)\n",
    "\n",
    "        return states, actions, rtgs, timesteps\n",
    "\n",
    "obss, actions, returns, done_idxs, rtgs, timesteps = create_dataset(args.num_buffers, args.num_steps, args.game, args.data_dir_prefix, args.trajectories_per_buffer)\n",
    "\n",
    "# set up logging\n",
    "logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    ")\n",
    "\n",
    "train_dataset = StateActionReturnDataset(obss, args.context_length*3, actions, done_idxs, rtgs, timesteps)\n",
    "\n",
    "mconf = GPTConfig(train_dataset.vocab_size, train_dataset.block_size,\n",
    "                  n_layer=6, n_head=8, n_embd=128, model_type=args.model_type, max_timestep=max(timesteps))\n",
    "model = GPT(mconf)\n",
    "\n",
    "# initialize a trainer instance and kick off training\n",
    "epochs = args.epochs\n",
    "tconf = TrainerConfig(max_epochs=epochs, batch_size=args.batch_size, learning_rate=6e-4,\n",
    "                      lr_decay=True, warmup_tokens=512*20, final_tokens=2*len(train_dataset)*args.context_length*3,\n",
    "                      num_workers=4, seed=args.seed, model_type=args.model_type, game=args.game, max_timestep=max(timesteps))\n",
    "trainer = Trainer(model, train_dataset, None, tconf)\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6063fafc-2d0c-4344-9fe9-5f84d3e8ef97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
