{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54ab4e32-3ce5-4249-af8a-f14f93e88d2f",
   "metadata": {},
   "source": [
    "# Discrete PPO\n",
    "This is the implementation of PPO + envpool for discrete action space using image inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bf0cf6-6d79-4384-a20f-3d0d5d848291",
   "metadata": {},
   "source": [
    "## Setup arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "949e1a15-c17b-498f-be92-cb52e4b750fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-10T01:35:38.342289Z",
     "iopub.status.busy": "2022-10-10T01:35:38.341957Z",
     "iopub.status.idle": "2022-10-10T01:35:38.732359Z",
     "shell.execute_reply": "2022-10-10T01:35:38.731697Z",
     "shell.execute_reply.started": "2022-10-10T01:35:38.342252Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f0521e7-34fc-49e5-84d7-326be26185a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-10T01:35:38.733567Z",
     "iopub.status.busy": "2022-10-10T01:35:38.733277Z",
     "iopub.status.idle": "2022-10-10T01:35:39.570741Z",
     "shell.execute_reply": "2022-10-10T01:35:39.570019Z",
     "shell.execute_reply.started": "2022-10-10T01:35:38.733532Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9490d11ecbe84688a143cf746a5a8171",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=1, description='seed', tooltip='seed of the experiment')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5e3dbb67b48435ba7dc200ead8848c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=True, description='torch-deterministic', indent=False, tooltip='if toggled, `torch.backends.cud…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a069d431cab4a2c895c52149d764ac1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=True, description='cuda', indent=False, tooltip='if toggled, cuda will be enabled by default')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abe5804382f74c12bfc8801cbdaf9126",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=False, description='capture-video', indent=False, tooltip='whether to capture videos of the age…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2776da327e5a48128eebb380e2bec81d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='Pong-v5', description='env-id', tooltip='the id of the environment')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b66e9ce18214c0f9320b8047abcf8f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=10000000, description='total-timesteps', tooltip='total timesteps of the experiments')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67a0e25911654a258956bf3df36ce7a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.00025, description='learning-rate', tooltip='the learning rate of the optimizer')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2d601acb60f42bc8a6b80d04a7d6722",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=8, description='num-envs', tooltip='the number of parallel game environments')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed73e3b0475545cab77090f7c04711f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=128, description='num-steps', tooltip='the number of steps to run in each environment per policy…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c325df96e0a4642987e364800956c29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=True, description='anneal-lr', indent=False, tooltip='Toggle learning rate annealing for policy…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b72fe7744e84a7cadc7b46532f28c10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.99, description='gamma', tooltip='the discount factor gamma')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de780beddc04488d961350fb9d1c4bc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.95, description='gae-lambda', tooltip='the lambda for the general advantage estimation')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67f3a96b0b6f40efb4709cdac7d43bbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=4, description='num-minibatches', tooltip='he number of mini-batches')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53ab06e138294625aafa1a5e1e50cd1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=4, description='update-epochs', tooltip='The K epochs to update the policy')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcabea72c69e41f2b4084aa497b5151e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=True, description='norm-adv', indent=False, tooltip='Toggles advantages normalization')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6400abc0e19a472bb00e6881eaf9abbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.1, description='clip-coef', tooltip='the surrogate clipping coefficient')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbce8bb808e64d308ea060a6ce9adcc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=True, description='clip-vloss', indent=False, tooltip='Toggles whether or not to use a clipped …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dc2f86f86204c4591a862f8a4a15c1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.01, description='ent-coef', tooltip='coefficient of the entropy')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9600bc60d4ae4f44a22ba8bbeae65d17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.5, description='vf-coef', tooltip='coefficient of the value function')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a424c1ac1f44aa5b2ff09ad34d7ea7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.5, description='max-grad-norm', tooltip='the maximum norm for the gradient clipping')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c24d2d51b3ff4591a5a274733bc22dde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0, description='target-kl', tooltip='the target KL divergence threshold')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output, Image\n",
    "\n",
    "import time\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch_rl.jupyter_utils import WidgetParser\n",
    "from torch_rl.utils import (\n",
    "    seed_everything,\n",
    "    mp4_to_gif,\n",
    "    MetricStore\n",
    ")\n",
    "\n",
    "\n",
    "parser = WidgetParser()\n",
    "parser.add_widget(widgets.IntText(\n",
    "    description='seed',\n",
    "    value=1,\n",
    "    tooltip=\"seed of the experiment\"\n",
    "))\n",
    "\n",
    "parser.add_widget(widgets.Checkbox(\n",
    "    description=\"torch-deterministic\",\n",
    "    value=True,\n",
    "    disabled=False,\n",
    "    indent=False,\n",
    "    tooltip=\"if toggled, `torch.backends.cudnn.deterministic=False`\"\n",
    "))\n",
    "\n",
    "parser.add_widget(widgets.Checkbox(\n",
    "    description=\"cuda\",\n",
    "    value=True,\n",
    "    disabled=False,\n",
    "    indent=False,\n",
    "    tooltip=\"if toggled, cuda will be enabled by default\"\n",
    "))\n",
    "\n",
    "parser.add_widget(widgets.Checkbox(\n",
    "    description=\"capture-video\",\n",
    "    value=False,\n",
    "    disabled=False,\n",
    "    indent=False,\n",
    "    tooltip=\"whether to capture videos of the agent performances (check out `videos` folder)\"\n",
    "))\n",
    "\n",
    "parser.add_widget(widgets.Text(\n",
    "    description=\"env-id\",\n",
    "    value=\"Pong-v5\",\n",
    "    tooltip=\"the id of the environment\"\n",
    "))\n",
    "\n",
    "parser.add_widget(widgets.IntText(\n",
    "    description='total-timesteps',\n",
    "    value=10000000,\n",
    "    tooltip=\"total timesteps of the experiments\"\n",
    "))\n",
    "\n",
    "parser.add_widget(widgets.FloatText(\n",
    "    description='learning-rate',\n",
    "    value=2.5e-4,\n",
    "    tooltip=\"the learning rate of the optimizer\"\n",
    "))\n",
    "\n",
    "parser.add_widget(widgets.IntText(\n",
    "    description='num-envs',\n",
    "    value=8,\n",
    "    tooltip=\"the number of parallel game environments\"\n",
    "))\n",
    "\n",
    "parser.add_widget(widgets.IntText(\n",
    "    description='num-steps',\n",
    "    value=128,\n",
    "    tooltip=\"the number of steps to run in each environment per policy rollout\"\n",
    "))\n",
    "\n",
    "parser.add_widget(widgets.Checkbox(\n",
    "    description=\"anneal-lr\",\n",
    "    value=True,\n",
    "    disabled=False,\n",
    "    indent=False,\n",
    "    tooltip=\"Toggle learning rate annealing for policy and value networks\"\n",
    "))\n",
    "\n",
    "parser.add_widget(widgets.FloatText(\n",
    "    description='gamma',\n",
    "    value=0.99,\n",
    "    tooltip=\"the discount factor gamma\"\n",
    "))\n",
    "\n",
    "parser.add_widget(widgets.FloatText(\n",
    "    description='gae-lambda',\n",
    "    value=0.95,\n",
    "    tooltip=\"the lambda for the general advantage estimation\"\n",
    "))\n",
    "\n",
    "parser.add_widget(widgets.IntText(\n",
    "    description='num-minibatches',\n",
    "    value=4,\n",
    "    tooltip=\"he number of mini-batches\"\n",
    "))\n",
    "\n",
    "parser.add_widget(widgets.IntText(\n",
    "    description='update-epochs',\n",
    "    value=4,\n",
    "    tooltip=\"The K epochs to update the policy\"\n",
    "))\n",
    "\n",
    "parser.add_widget(widgets.Checkbox(\n",
    "    description=\"norm-adv\",\n",
    "    value=True,\n",
    "    disabled=False,\n",
    "    indent=False,\n",
    "    tooltip=\"Toggles advantages normalization\"\n",
    "))\n",
    "\n",
    "parser.add_widget(widgets.FloatText(\n",
    "    description='clip-coef',\n",
    "    value=0.1,\n",
    "    tooltip=\"the surrogate clipping coefficient\"\n",
    "))\n",
    "\n",
    "parser.add_widget(widgets.Checkbox(\n",
    "    description=\"clip-vloss\",\n",
    "    value=True,\n",
    "    disabled=False,\n",
    "    indent=False,\n",
    "    tooltip=\"Toggles whether or not to use a clipped loss for the value function, as per the paper\"\n",
    "))\n",
    "\n",
    "parser.add_widget(widgets.FloatText(\n",
    "    description='ent-coef',\n",
    "    value=0.01,\n",
    "    tooltip=\"coefficient of the entropy\"\n",
    "))\n",
    "\n",
    "parser.add_widget(widgets.FloatText(\n",
    "    description='vf-coef',\n",
    "    value=0.5,\n",
    "    tooltip=\"coefficient of the value function\"\n",
    "))\n",
    "\n",
    "parser.add_widget(widgets.FloatText(\n",
    "    description='max-grad-norm',\n",
    "    value=0.5,\n",
    "    tooltip=\"the maximum norm for the gradient clipping\"\n",
    "))\n",
    "\n",
    "parser.add_widget(widgets.FloatText(\n",
    "    description='target-kl',\n",
    "    value=0,\n",
    "    tooltip=\"the target KL divergence threshold\"\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c0283d-5bd9-431e-9a7e-a0a83f5dfc6d",
   "metadata": {},
   "source": [
    "### Parse inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e95c57e4-126e-4cac-9454-6af1863c29d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-10T01:35:39.572028Z",
     "iopub.status.busy": "2022-10-10T01:35:39.571574Z",
     "iopub.status.idle": "2022-10-10T01:35:39.590973Z",
     "shell.execute_reply": "2022-10-10T01:35:39.590267Z",
     "shell.execute_reply.started": "2022-10-10T01:35:39.572007Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'using device cuda'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "args = parser.parse()\n",
    "args.batch_size = int(args.num_envs * args.num_steps)\n",
    "args.minibatch_size = int(args.batch_size // args.num_minibatches)\n",
    "\n",
    "run_name = f\"{args.env_id}__ppo__{args.seed}__{int(time.time())}\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and args.cuda else \"cpu\")\n",
    "seed_everything(args.seed, args.torch_deterministic)\n",
    "display(f\"using device {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b53fde3-b2aa-45b4-8ac1-f28e6b21b68e",
   "metadata": {},
   "source": [
    "## Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e30e563d-4eb3-4626-941a-f156979190ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-10T01:35:39.592134Z",
     "iopub.status.busy": "2022-10-10T01:35:39.591894Z",
     "iopub.status.idle": "2022-10-10T01:35:39.826879Z",
     "shell.execute_reply": "2022-10-10T01:35:39.826167Z",
     "shell.execute_reply.started": "2022-10-10T01:35:39.592114Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.12) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "\n",
    "class RecordEpisodeStatistics(gym.Wrapper):\n",
    "    def __init__(self, env, deque_size=100):\n",
    "        super().__init__(env)\n",
    "        self.num_envs = getattr(env, \"num_envs\", 1)\n",
    "        self.episode_returns = None\n",
    "        self.episode_lengths = None\n",
    "        # get if the env has lives\n",
    "        self.has_lives = False\n",
    "        env.reset()\n",
    "        info = env.step(np.zeros(self.num_envs, dtype=int))[-1]\n",
    "        if info[\"lives\"].sum() > 0:\n",
    "            self.has_lives = True\n",
    "            print(\"env has lives\")\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        observations = super().reset(**kwargs)\n",
    "        self.episode_returns = np.zeros(self.num_envs, dtype=np.float32)\n",
    "        self.episode_lengths = np.zeros(self.num_envs, dtype=np.int32)\n",
    "        self.lives = np.zeros(self.num_envs, dtype=np.int32)\n",
    "        self.returned_episode_returns = np.zeros(self.num_envs, dtype=np.float32)\n",
    "        self.returned_episode_lengths = np.zeros(self.num_envs, dtype=np.int32)\n",
    "        return observations\n",
    "\n",
    "    def step(self, action):\n",
    "        observations, rewards, dones, infos = super().step(action)\n",
    "        self.episode_returns += infos[\"reward\"]\n",
    "        self.episode_lengths += 1\n",
    "        self.returned_episode_returns[:] = self.episode_returns\n",
    "        self.returned_episode_lengths[:] = self.episode_lengths\n",
    "        all_lives_exhausted = infos[\"lives\"] == 0\n",
    "        if self.has_lives:\n",
    "            self.episode_returns *= 1 - all_lives_exhausted\n",
    "            self.episode_lengths *= 1 - all_lives_exhausted\n",
    "        else:\n",
    "            self.episode_returns *= 1 - dones\n",
    "            self.episode_lengths *= 1 - dones\n",
    "        infos[\"r\"] = self.returned_episode_returns\n",
    "        infos[\"l\"] = self.returned_episode_lengths\n",
    "        return (\n",
    "            observations,\n",
    "            rewards,\n",
    "            dones,\n",
    "            infos,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6ef50e-adfe-47c7-9745-6105462dd25c",
   "metadata": {},
   "source": [
    "### Ensure environment compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39eec428-f195-40b3-8766-780b1574dd2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-10T01:35:39.828038Z",
     "iopub.status.busy": "2022-10-10T01:35:39.827796Z",
     "iopub.status.idle": "2022-10-10T01:35:40.376459Z",
     "shell.execute_reply": "2022-10-10T01:35:40.375776Z",
     "shell.execute_reply.started": "2022-10-10T01:35:39.828018Z"
    }
   },
   "outputs": [],
   "source": [
    "import envpool\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "envs = envpool.make(\n",
    "    args.env_id,\n",
    "    env_type=\"gym\",\n",
    "    num_envs=args.num_envs,\n",
    "    episodic_life=True,\n",
    "    reward_clip=True,\n",
    "    seed=args.seed,\n",
    ")\n",
    "envs.num_envs = args.num_envs\n",
    "envs.single_action_space = envs.action_space\n",
    "envs.single_observation_space = envs.observation_space\n",
    "envs = RecordEpisodeStatistics(envs)\n",
    "assert isinstance(envs.action_space, gym.spaces.Discrete), \"only discrete action space is supported\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d564f66b-d7f8-4210-a875-6fe61ec1a11d",
   "metadata": {},
   "source": [
    "### Environment examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6f7cad5-0983-4d29-8cb8-ee5fb11b34f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-10T01:35:40.378768Z",
     "iopub.status.busy": "2022-10-10T01:35:40.378440Z",
     "iopub.status.idle": "2022-10-10T01:35:40.627469Z",
     "shell.execute_reply": "2022-10-10T01:35:40.626944Z",
     "shell.execute_reply.started": "2022-10-10T01:35:40.378749Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.7.5+db37282)\n",
      "[Powered by Stella]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tmp_env = gym.make(f\"ALE/{args.env_id}\")\n",
    "ob = tmp_env.reset()\n",
    "\n",
    "plt.title(args.env_id)\n",
    "plt.imshow(tmp_env.render(\"rgb_array\"))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b012f3ae-facb-4055-a780-48c887b8d17f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-10T01:35:40.628455Z",
     "iopub.status.busy": "2022-10-10T01:35:40.628212Z",
     "iopub.status.idle": "2022-10-10T01:35:40.650279Z",
     "shell.execute_reply": "2022-10-10T01:35:40.649490Z",
     "shell.execute_reply.started": "2022-10-10T01:35:40.628435Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, (210, 160, 3))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action = tmp_env.action_space.sample()\n",
    "ob, reward, done, info = tmp_env.step(action)\n",
    "action, ob.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ed0dd4-3fa0-4373-ba59-43973f0fd3cc",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae74d95f-7c18-45ad-b11e-9b917cc1ab48",
   "metadata": {},
   "source": [
    "### DQN training Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0cfbf718-8113-42f3-a808-aea7e10dd036",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-10T01:35:40.651652Z",
     "iopub.status.busy": "2022-10-10T01:35:40.651416Z",
     "iopub.status.idle": "2022-10-10T01:35:42.163858Z",
     "shell.execute_reply": "2022-10-10T01:35:42.163180Z",
     "shell.execute_reply.started": "2022-10-10T01:35:40.651632Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ppo import PPO, PPOTrainingWrapper\n",
    "agent = PPO(\n",
    "    envs\n",
    ").to(device)\n",
    "\n",
    "wrapper = PPOTrainingWrapper(\n",
    "    model = agent,\n",
    "    env=envs,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c1cf3c-7a48-4a80-8ebe-4454f0ee6725",
   "metadata": {},
   "source": [
    "### Setup plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67ed594b-c506-4c7d-866c-2a26b0bbecbe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-10T01:35:42.164985Z",
     "iopub.status.busy": "2022-10-10T01:35:42.164732Z",
     "iopub.status.idle": "2022-10-10T01:35:42.184574Z",
     "shell.execute_reply": "2022-10-10T01:35:42.183916Z",
     "shell.execute_reply.started": "2022-10-10T01:35:42.164966Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_metrics(ms, fontsize=12):\n",
    "    def _plot(ax, metric_name, color='blue'):\n",
    "        ax.set_title(metric_name, fontsize=fontsize)\n",
    "        ax.plot(ms.get_metric(metric_name), color=color)\n",
    "\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows=2, ncols=2)\n",
    "    # clear_output(wait=True)\n",
    "\n",
    "    _plot(ax1, \"episodic_return\", 'blue')\n",
    "    _plot(ax2, \"episodic_length\", 'black')\n",
    "    _plot(ax3, \"value_loss\", 'orange')\n",
    "    _plot(ax4, \"policy_loss\", 'red')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b7d082-ee8c-4343-97c9-d8a4cb9e3029",
   "metadata": {},
   "source": [
    "### DQN training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b30a56-e50d-4752-9339-87a1b64e450c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-10T01:35:42.185783Z",
     "iopub.status.busy": "2022-10-10T01:35:42.185538Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'9963520/10000000 ( 99% )'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "\n",
    "optimizer = wrapper.get_optimizer(lr=1e-5)\n",
    "ts = wrapper.get_storage(args.num_steps, args.num_envs)\n",
    "\n",
    "ms = MetricStore()\n",
    "\n",
    "global_step = 0\n",
    "start_time = time.time()\n",
    "next_obs = torch.Tensor(envs.reset()).to(device)\n",
    "next_done = torch.zeros(args.num_envs).to(device)\n",
    "num_updates = args.total_timesteps // args.batch_size\n",
    "\n",
    "for update in range(1, num_updates + 1):\n",
    "    if args.anneal_lr:\n",
    "        lrnow = wrapper.anneal_lr(args.learning_rate, update, num_updates)\n",
    "        optimizer.param_groups[0][\"lr\"] = lrnow\n",
    "\n",
    "    for step in range(0, args.num_steps):\n",
    "        global_step += 1 * args.num_envs\n",
    "        ts.obs[step] = next_obs\n",
    "        ts.dones[step] = next_done\n",
    "\n",
    "        with torch.no_grad():\n",
    "            action, logprob, _, value = agent.get_action_and_value(next_obs)\n",
    "            ts.values[step] = value.flatten()\n",
    "        ts.actions[step] = action\n",
    "        ts.logprobs[step] = logprob\n",
    "\n",
    "        next_obs, reward, done, info = envs.step(action.cpu().numpy())\n",
    "        ts.rewards[step] = torch.tensor(reward).to(device).view(-1)\n",
    "        next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor(done).to(device)\n",
    "\n",
    "        for idx, d in enumerate(done):\n",
    "            if d and info[\"lives\"][idx] == 0:\n",
    "                ms.log_metric(\"episodic_return\", info[\"r\"][idx], global_step)\n",
    "                ms.log_metric(\"episodic_length\", info[\"l\"][idx], global_step)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        advantages = wrapper.get_advantages(args.num_steps, next_done, next_obs, args.gamma, args.gae_lambda, ts)\n",
    "        returns = advantages + ts.values\n",
    "\n",
    "    b_obs = ts.obs.reshape((-1,) + envs.single_observation_space.shape)\n",
    "    b_logprobs = ts.logprobs.reshape(-1)\n",
    "    b_actions = ts.actions.reshape((-1,) + envs.single_action_space.shape)\n",
    "    b_advantages = advantages.reshape(-1)\n",
    "    b_returns = returns.reshape(-1)\n",
    "    b_values = ts.values.reshape(-1)\n",
    "\n",
    "    # Optimizing the policy and value network\n",
    "    b_inds = np.arange(args.batch_size)\n",
    "    for epoch in range(args.update_epochs):\n",
    "        np.random.shuffle(b_inds)\n",
    "        for start in range(0, args.batch_size, args.minibatch_size):\n",
    "            end = start + args.minibatch_size\n",
    "            mb_inds = b_inds[start:end]\n",
    "\n",
    "            loss, approx_kl, losses = wrapper.train_step(\n",
    "                b_obs[mb_inds],\n",
    "                b_actions.long()[mb_inds],\n",
    "                b_logprobs[mb_inds],\n",
    "                b_advantages[mb_inds],\n",
    "                b_returns[mb_inds],\n",
    "                b_values[mb_inds],\n",
    "                args.norm_adv,\n",
    "                args.clip_coef,\n",
    "                args.clip_vloss,\n",
    "                args.ent_coef,\n",
    "                args.vf_coef\n",
    "            )\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            wrapper.clip_grad_norm(args.max_grad_norm)\n",
    "            optimizer.step()\n",
    "\n",
    "        if args.target_kl is not None and args.target_kl > 0:\n",
    "            if approx_kl > args.target_kl:\n",
    "                break\n",
    "\n",
    "    y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n",
    "    var_y = np.var(y_true)\n",
    "    explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
    "\n",
    "    (pg_loss, v_loss, entropy_loss) = losses\n",
    "\n",
    "    ms.log_metric(\"learning_rate\", optimizer.param_groups[0][\"lr\"], global_step)\n",
    "    ms.log_metric(\"value_loss\", v_loss.item(), global_step)\n",
    "    ms.log_metric(\"policy_loss\", pg_loss.item(), global_step)\n",
    "    ms.log_metric(\"entropy\", entropy_loss.item(), global_step)\n",
    "    ms.log_metric(\"approx_kl\", approx_kl.item(), global_step)\n",
    "    ms.log_metric(\"explained_variance\", explained_var, global_step)\n",
    "\n",
    "    if update % 10 == 0:\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        current_step = update * args.batch_size\n",
    "        cureent_perc = current_step * 100 // args.total_timesteps \n",
    "        print(f\"{current_step}/{args.total_timesteps} ( {cureent_perc}% )\")\n",
    "        plot_metrics(ms)\n",
    "\n",
    "envs.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f37e7f-dc03-4ba1-9717-67aeabda381f",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11189f2e-b627-4fd5-8669-f4a9f3b96cf8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run_test_name = f\"test-{run_name}\"\n",
    "test_env = envpool.make(\n",
    "    args.env_id,\n",
    "    env_type=\"gym\",\n",
    "    num_envs=1,\n",
    "    episodic_life=True,\n",
    "    reward_clip=True,\n",
    ")\n",
    "test_env.num_envs = args.num_envs\n",
    "test_env.single_action_space = test_env.action_space\n",
    "test_env.single_observation_space = test_env.observation_space\n",
    "test_env = RecordEpisodeStatistics(test_env)\n",
    "test_env = gym.wrappers.RecordVideo(test_env, f\"videos/{run_test_name}\")\n",
    "\n",
    "next_obs = test_env.reset()\n",
    "while True:\n",
    "    action, _, _, _ = agent.get_action_and_value(torch.Tensor(next_obs).to(device))\n",
    "    next_obs, reward, done, info = test_env.step(action.cpu().numpy())\n",
    "\n",
    "    if done[0] and info[\"lives\"][0] == 0:\n",
    "        print(f\"episodic_return={info['r'][0]}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1491d9-3a95-4dcb-922e-599296894526",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
