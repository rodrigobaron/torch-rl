{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0a0e7fc-4cc1-475d-8955-a72a44d14a69",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-03T12:17:27.298220Z",
     "iopub.status.busy": "2022-10-03T12:17:27.297782Z",
     "iopub.status.idle": "2022-10-03T12:18:59.244312Z",
     "shell.execute_reply": "2022-10-03T12:18:59.241606Z",
     "shell.execute_reply.started": "2022-10-03T12:17:27.298158Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 1)) (1.12.0)\n",
      "Collecting atari-py==0.2.5\n",
      "  Using cached atari-py-0.2.5.tar.gz (790 kB)\n",
      "Processing /root/.cache/pip/wheels/27/6d/b3/a3a6e10704795c9b9000f1ab2dc480dfe7bed42f5972806e73/gym-0.21.0-py3-none-any.whl\n",
      "Collecting opencv-python==4.6.0.66\n",
      "  Using cached opencv_python-4.6.0.66-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (60.9 MB)\n",
      "Collecting envpool==0.4.5\n",
      "  Using cached envpool-0.4.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (22.8 MB)\n",
      "Collecting pyglet==1.5.26\n",
      "  Using cached pyglet-1.5.26-py3-none-any.whl (1.1 MB)\n",
      "Collecting stable_baselines3\n",
      "  Using cached stable_baselines3-1.6.1-py3-none-any.whl (180 kB)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch->-r requirements.txt (line 1)) (4.3.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from atari-py==0.2.5->-r requirements.txt (line 2)) (1.23.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from atari-py==0.2.5->-r requirements.txt (line 2)) (1.16.0)\n",
      "Collecting cloudpickle>=1.2.0\n",
      "  Using cached cloudpickle-2.2.0-py3-none-any.whl (25 kB)\n",
      "Collecting autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"\n",
      "  Using cached AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
      "Collecting ale-py~=0.7.1; extra == \"atari\"\n",
      "  Using cached ale_py-0.7.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "Collecting types-protobuf>=3.17.3\n",
      "  Using cached types_protobuf-3.20.4-py3-none-any.whl (60 kB)\n",
      "Collecting dm-env>=1.4\n",
      "  Using cached dm_env-1.5-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from stable_baselines3->-r requirements.txt (line 9)) (1.5.0)\n",
      "Collecting matplotlib\n",
      "  Using cached matplotlib-3.6.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (9.4 MB)\n",
      "Collecting tqdm\n",
      "  Using cached tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
      "Requirement already satisfied: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.8/dist-packages (from autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license,atari]==0.21.0->-r requirements.txt (line 3)) (5.9.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license,atari]==0.21.0->-r requirements.txt (line 3)) (2.28.1)\n",
      "Collecting click\n",
      "  Using cached click-8.1.3-py3-none-any.whl (96 kB)\n",
      "Processing /root/.cache/pip/wheels/51/08/c5/28b973078691a3f8baf99fcaec1ed8f0e05ef6e54d2390212c/AutoROM.accept_rom_license-0.4.2-py3-none-any.whl\n",
      "Requirement already satisfied: importlib-metadata>=4.10.0; python_version < \"3.10\" in /usr/local/lib/python3.8/dist-packages (from ale-py~=0.7.1; extra == \"atari\"->gym[accept-rom-license,atari]==0.21.0->-r requirements.txt (line 3)) (5.0.0)\n",
      "Collecting absl-py\n",
      "  Using cached absl_py-1.2.0-py3-none-any.whl (123 kB)\n",
      "Collecting dm-tree\n",
      "  Using cached dm_tree-0.1.7-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (142 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.8/dist-packages (from pandas->stable_baselines3->-r requirements.txt (line 9)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas->stable_baselines3->-r requirements.txt (line 9)) (2022.4)\n",
      "Collecting cycler>=0.10\n",
      "  Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Using cached kiwisolver-1.4.4-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.2 MB)\n",
      "Collecting contourpy>=1.0.1\n",
      "  Using cached contourpy-1.0.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (295 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib->stable_baselines3->-r requirements.txt (line 9)) (21.3)\n",
      "Collecting pillow>=6.2.0\n",
      "  Using cached Pillow-9.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->stable_baselines3->-r requirements.txt (line 9)) (3.0.9)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Using cached fonttools-4.37.4-py3-none-any.whl (960 kB)\n",
      "Requirement already satisfied: zipp>=3.1.0; python_version < \"3.10\" in /usr/local/lib/python3.8/dist-packages (from importlib-resources; python_version < \"3.9\"->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license,atari]==0.21.0->-r requirements.txt (line 3)) (3.8.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license,atari]==0.21.0->-r requirements.txt (line 3)) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license,atari]==0.21.0->-r requirements.txt (line 3)) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license,atari]==0.21.0->-r requirements.txt (line 3)) (1.26.12)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license,atari]==0.21.0->-r requirements.txt (line 3)) (2.1.1)\n",
      "Building wheels for collected packages: atari-py\n",
      "  Building wheel for atari-py (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for atari-py: filename=atari_py-0.2.5-cp38-cp38-linux_x86_64.whl size=2811367 sha256=d4e521b21e627a70d748a4423d540539b8234fa5f176d8f26d0e18b6ae1c1f19\n",
      "  Stored in directory: /root/.cache/pip/wheels/63/40/77/805181145a2828dc2570ae7c9b8699157f0cec9321b10cd2e0\n",
      "Successfully built atari-py\n",
      "Installing collected packages: atari-py, cloudpickle, tqdm, click, AutoROM.accept-rom-license, autorom, ale-py, gym, opencv-python, types-protobuf, absl-py, dm-tree, dm-env, envpool, pyglet, cycler, kiwisolver, contourpy, pillow, fonttools, matplotlib, stable-baselines3\n",
      "Successfully installed AutoROM.accept-rom-license-0.4.2 absl-py-1.2.0 ale-py-0.7.5 atari-py-0.2.5 autorom-0.4.2 click-8.1.3 cloudpickle-2.2.0 contourpy-1.0.5 cycler-0.11.0 dm-env-1.5 dm-tree-0.1.7 envpool-0.4.5 fonttools-4.37.4 gym-0.21.0 kiwisolver-1.4.4 matplotlib-3.6.0 opencv-python-4.6.0.66 pillow-9.2.0 pyglet-1.5.26 stable-baselines3-1.6.1 tqdm-4.64.1 types-protobuf-3.20.4\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb08f0a9-12b3-495a-b31c-3de241fa906e",
   "metadata": {},
   "source": [
    "## Setup arguments\n",
    "\n",
    "This automatically parse arguments as jupyter inputs making it more user friendly :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed21c841-2841-468e-8705-d2fe7528cf71",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-03T12:18:59.251615Z",
     "iopub.status.busy": "2022-10-03T12:18:59.250764Z",
     "iopub.status.idle": "2022-10-03T12:18:59.406234Z",
     "shell.execute_reply": "2022-10-03T12:18:59.405543Z",
     "shell.execute_reply.started": "2022-10-03T12:18:59.251519Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31ec3fbad6e04438a23ef4ccbed976a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=1, description='seed')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "212c91165f334287950e0a0832143a15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=True, description='torch_deterministic', indent=False)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e28a87b45c84963aed543fc016b16ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=True, description='cuda', indent=False)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d67ee13f79564372ba16476436f05cc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=False, description='capture_video', indent=False)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b13cdfc0a2804397ac70d0c1e6afaf00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='BreakoutNoFrameskip-v4', description='env_id')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54016b86a4fa4028be8e1fc4c1263be2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=10000000, description='total_timesteps')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1cf950632474914ab002e17d4674a63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0001, description='learning_rate')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12ec31685862408db39349f73e3794ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=1000000, description='buffer_size')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad8268df36ff49b9be28af4ac59a4981",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.99, description='gamma')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59259ad8660748ae82b4234c4f00b12d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=1000, description='target_network_frequency')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e6e311a499d4eeaa013b8875baa9b2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=32, description='batch_size')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a412192aff704a10af0a05aeed717538",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=1.0, description='start_e')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "771a090116de49d0a11d52b2b67ecad9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.01, description='end_e')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcb98af3ae0e4a6ab0ba067e2367ea2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.1, description='exploration_fraction')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "153c6a88f8534f77974226863a9211c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=80000, description='learning_starts')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cef5d9732d1848e18b332750c415e9c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=4, description='train_frequency')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from distutils.util import strtobool\n",
    "\n",
    "from jupyter_utils import JupyterArgumentParser\n",
    "\n",
    "\n",
    "def get_args():\n",
    "    parser = JupyterArgumentParser()\n",
    "    parser.add_argument(\"--seed\", type=int, default=1,\n",
    "        help=\"seed of the experiment\")\n",
    "    parser.add_argument(\"--torch-deterministic\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n",
    "        help=\"if toggled, `torch.backends.cudnn.deterministic=False`\")\n",
    "    parser.add_argument(\"--cuda\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n",
    "        help=\"if toggled, cuda will be enabled by default\")\n",
    "    parser.add_argument(\"--capture-video\", type=lambda x: bool(strtobool(x)), default=False, nargs=\"?\", const=True,\n",
    "        help=\"whether to capture videos of the agent performances (check out `videos` folder)\")\n",
    "    parser.add_argument(\"--env-id\", type=str, default=\"BreakoutNoFrameskip-v4\",\n",
    "        help=\"the id of the environment\")\n",
    "    parser.add_argument(\"--total-timesteps\", type=int, default=10000000,\n",
    "        help=\"total timesteps of the experiments\")\n",
    "    parser.add_argument(\"--learning-rate\", type=float, default=1e-4,\n",
    "        help=\"the learning rate of the optimizer\")\n",
    "    parser.add_argument(\"--buffer-size\", type=int, default=1000000,\n",
    "        help=\"the replay memory buffer size\")\n",
    "    parser.add_argument(\"--gamma\", type=float, default=0.99,\n",
    "        help=\"the discount factor gamma\")\n",
    "    parser.add_argument(\"--target-network-frequency\", type=int, default=1000,\n",
    "        help=\"the timesteps it takes to update the target network\")\n",
    "    parser.add_argument(\"--batch-size\", type=int, default=32,\n",
    "        help=\"the batch size of sample from the reply memory\")\n",
    "    parser.add_argument(\"--start-e\", type=float, default=1,\n",
    "        help=\"the starting epsilon for exploration\")\n",
    "    parser.add_argument(\"--end-e\", type=float, default=0.01,\n",
    "        help=\"the ending epsilon for exploration\")\n",
    "    parser.add_argument(\"--exploration-fraction\", type=float, default=0.10,\n",
    "        help=\"the fraction of `total-timesteps` it takes from start-e to go end-e\")\n",
    "    parser.add_argument(\"--learning-starts\", type=int, default=80000,\n",
    "        help=\"timestep to start learning\")\n",
    "    parser.add_argument(\"--train-frequency\", type=int, default=4,\n",
    "        help=\"the frequency of training\")\n",
    "    return parser\n",
    "\n",
    "\n",
    "parser = get_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49de7812-47cd-4b73-a015-fa3bc6decb72",
   "metadata": {},
   "source": [
    "## Setup wrappers\n",
    "\n",
    "Use preprocessing wrappers to transfom the inputs according to baselines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eebc8f65-d3ba-40f5-8c0e-a6486e3e5074",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-03T12:18:59.407269Z",
     "iopub.status.busy": "2022-10-03T12:18:59.407013Z",
     "iopub.status.idle": "2022-10-03T12:19:01.922200Z",
     "shell.execute_reply": "2022-10-03T12:19:01.921422Z",
     "shell.execute_reply.started": "2022-10-03T12:18:59.407248Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import gym\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from atari import make_atari_env\n",
    "from utils import seed_everything\n",
    "from buffers import ReplayBuffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08610d0-aa6c-4dbe-8cf1-8b088e560c9b",
   "metadata": {},
   "source": [
    "Parse experiment arguments.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9081e3e1-623c-40d0-9eb2-ecf339fa0328",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-03T12:19:01.923586Z",
     "iopub.status.busy": "2022-10-03T12:19:01.923259Z",
     "iopub.status.idle": "2022-10-03T12:19:01.928011Z",
     "shell.execute_reply": "2022-10-03T12:19:01.927396Z",
     "shell.execute_reply.started": "2022-10-03T12:19:01.923563Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|param=value|\n",
      "|-|-|\n",
      "|seed=1|\n",
      "|torch_deterministic=True|\n",
      "|cuda=True|\n",
      "|capture_video=False|\n",
      "|env_id=BreakoutNoFrameskip-v4|\n",
      "|total_timesteps=10000000|\n",
      "|learning_rate=0.0001|\n",
      "|buffer_size=1000000|\n",
      "|gamma=0.99|\n",
      "|target_network_frequency=1000|\n",
      "|batch_size=32|\n",
      "|start_e=1.0|\n",
      "|end_e=0.01|\n",
      "|exploration_fraction=0.1|\n",
      "|learning_starts=80000|\n",
      "|train_frequency=4|\n"
     ]
    }
   ],
   "source": [
    "args = parser.parse_args()\n",
    "print(\"|param=value|\\n|-|-|\\n%s\" % (\"\\n\".join([f\"|{key}={value}|\" for key, value in vars(args).items()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b417f3a-206d-4624-85fb-62789374bb76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-03T12:19:01.929067Z",
     "iopub.status.busy": "2022-10-03T12:19:01.928789Z",
     "iopub.status.idle": "2022-10-03T12:19:01.935741Z",
     "shell.execute_reply": "2022-10-03T12:19:01.935173Z",
     "shell.execute_reply.started": "2022-10-03T12:19:01.929046Z"
    }
   },
   "outputs": [],
   "source": [
    "run_name = f\"{args.env_id}__dqn__{args.seed}__{int(time.time())}\" \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and args.cuda else \"cpu\")\n",
    "seed_everything(args.seed, args.torch_deterministic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fffc90-d838-4b3d-96f0-683ea7ddd01d",
   "metadata": {},
   "source": [
    "## Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb7cba57-366e-412e-8eb6-42b9c7fec5ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-03T12:19:01.936772Z",
     "iopub.status.busy": "2022-10-03T12:19:01.936531Z",
     "iopub.status.idle": "2022-10-03T12:19:02.226978Z",
     "shell.execute_reply": "2022-10-03T12:19:02.226252Z",
     "shell.execute_reply.started": "2022-10-03T12:19:01.936752Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.7.5+db37282)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "envs = gym.vector.SyncVectorEnv([make_atari_env(args.env_id, args.seed, 0, args.capture_video, run_name)])\n",
    "assert isinstance(envs.single_action_space, gym.spaces.Discrete), \"only discrete action space is supported\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d229b6b-efac-43bd-818f-74c15f118f9b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-03T03:17:34.254488Z",
     "iopub.status.busy": "2022-10-03T03:17:34.254047Z",
     "iopub.status.idle": "2022-10-03T03:17:34.270359Z",
     "shell.execute_reply": "2022-10-03T03:17:34.269512Z",
     "shell.execute_reply.started": "2022-10-03T03:17:34.254464Z"
    }
   },
   "source": [
    "## Few examples\n",
    "\n",
    "Show few env steps and the transformation input.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585b73cc-a012-40a0-a735-65f4090f723a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "441336c2-97c3-49c9-b12a-21cfd4a06df3",
   "metadata": {},
   "source": [
    "## Define Q Network\n",
    "\n",
    "The Q network is .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba819a6e-0552-4d22-bcca-768c22651999",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-03T12:19:02.229302Z",
     "iopub.status.busy": "2022-10-03T12:19:02.229014Z",
     "iopub.status.idle": "2022-10-03T12:19:02.234938Z",
     "shell.execute_reply": "2022-10-03T12:19:02.233988Z",
     "shell.execute_reply.started": "2022-10-03T12:19:02.229282Z"
    }
   },
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Conv2d(4, 32, 8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3136, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, env.single_action_space.n),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x / 255.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398bf469-74ef-4f94-9218-8a94369e5f70",
   "metadata": {},
   "source": [
    "## Setup DQN training\n",
    "\n",
    "Setup QNetwork and TargetNetwork along with ReplayBuffer, Optimization and schedules.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b09d21e9-a479-4470-ae1d-24ff5b806b48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-03T12:19:02.235997Z",
     "iopub.status.busy": "2022-10-03T12:19:02.235763Z",
     "iopub.status.idle": "2022-10-03T12:19:02.240228Z",
     "shell.execute_reply": "2022-10-03T12:19:02.239426Z",
     "shell.execute_reply.started": "2022-10-03T12:19:02.235976Z"
    }
   },
   "outputs": [],
   "source": [
    "def linear_schedule(start_e: float, end_e: float, duration: int, t: int):\n",
    "    slope = (end_e - start_e) / duration\n",
    "    return max(slope * t + start_e, end_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a025abed-7847-4939-b9d5-beae1310078a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-03T12:19:02.241099Z",
     "iopub.status.busy": "2022-10-03T12:19:02.240834Z",
     "iopub.status.idle": "2022-10-03T12:19:04.871216Z",
     "shell.execute_reply": "2022-10-03T12:19:04.870446Z",
     "shell.execute_reply.started": "2022-10-03T12:19:02.241078Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/app/notebooks/buffers.py:374: UserWarning: This system does not have apparently enough memory to store the complete replay buffer 28.24GB > 4.98GB\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "q_network = QNetwork(envs).to(device)\n",
    "optimizer = optim.Adam(q_network.parameters(), lr=args.learning_rate)\n",
    "target_network = QNetwork(envs).to(device)\n",
    "target_network.load_state_dict(q_network.state_dict())\n",
    "\n",
    "rb = ReplayBuffer(\n",
    "    args.buffer_size,\n",
    "    envs.single_observation_space,\n",
    "    envs.single_action_space,\n",
    "    device,\n",
    "    optimize_memory_usage=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dde87068-f252-4635-ba0e-5b4e230fe0b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-03T12:19:04.872508Z",
     "iopub.status.busy": "2022-10-03T12:19:04.872242Z",
     "iopub.status.idle": "2022-10-03T12:19:04.882194Z",
     "shell.execute_reply": "2022-10-03T12:19:04.881388Z",
     "shell.execute_reply.started": "2022-10-03T12:19:04.872488Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_step(obs, global_step):\n",
    "    # ALGO LOGIC: put action logic here\n",
    "    epsilon = linear_schedule(args.start_e, args.end_e, args.exploration_fraction * args.total_timesteps, global_step)\n",
    "    if random.random() < epsilon:\n",
    "        actions = np.array([envs.single_action_space.sample() for _ in range(envs.num_envs)])\n",
    "    else:\n",
    "        q_values = q_network(torch.Tensor(obs).to(device))\n",
    "        actions = torch.argmax(q_values, dim=1).cpu().numpy()\n",
    "\n",
    "    # TRY NOT TO MODIFY: execute the game and log data.\n",
    "    next_obs, rewards, dones, infos = envs.step(actions)\n",
    "\n",
    "    loss = None\n",
    "    metrics = {}\n",
    "\n",
    "    # TRY NOT TO MODIFY: record rewards for plotting purposes\n",
    "    for info in infos:\n",
    "        if \"episode\" in info.keys():\n",
    "            # print(f\"global_step={global_step}, episodic_return={info['episode']['r']}\")\n",
    "            metrics['episodic_return'] = info[\"episode\"][\"r\"]\n",
    "            # writer.add_scalar(\"charts/episodic_return\", info[\"episode\"][\"r\"], global_step)\n",
    "            # writer.add_scalar(\"charts/episodic_length\", info[\"episode\"][\"l\"], global_step)\n",
    "            # writer.add_scalar(\"charts/epsilon\", epsilon, global_step)\n",
    "            break\n",
    "\n",
    "    # TRY NOT TO MODIFY: save data to reply buffer; handle `terminal_observation`\n",
    "    real_next_obs = next_obs.copy()\n",
    "    for idx, d in enumerate(dones):\n",
    "        if \"terminal_observation\" in infos[idx].keys() and d:\n",
    "            real_next_obs[idx] = infos[idx][\"terminal_observation\"]\n",
    "    rb.add(obs, real_next_obs, actions, rewards, dones)\n",
    "\n",
    "    # TRY NOT TO MODIFY: CRUCIAL step easy to overlook\n",
    "    # obs = next_obs\n",
    "\n",
    "    # ALGO LOGIC: training.\n",
    "    if global_step > args.learning_starts and global_step % args.train_frequency == 0:\n",
    "        data = rb.sample(args.batch_size)\n",
    "        with torch.no_grad():\n",
    "            target_max, _ = target_network(data.next_observations).max(dim=1)\n",
    "            td_target = data.rewards.flatten() + args.gamma * target_max * (1 - data.dones.flatten())\n",
    "        old_val = q_network(data.observations).gather(1, data.actions).squeeze()\n",
    "        loss = F.mse_loss(td_target, old_val)\n",
    "\n",
    "        # if global_step % 100 == 0:\n",
    "            # writer.add_scalar(\"losses/td_loss\", loss, global_step)\n",
    "            # writer.add_scalar(\"losses/q_values\", old_val.mean().item(), global_step)\n",
    "            # print(\"SPS:\", int(global_step / (time.time() - start_time)))\n",
    "            # writer.add_scalar(\"charts/SPS\", int(global_step / (time.time() - start_time)), global_step)\n",
    "\n",
    "        # optimize the model\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # update the target network\n",
    "        if global_step % args.target_network_frequency == 0:\n",
    "            target_network.load_state_dict(q_network.state_dict())\n",
    "    return next_obs, loss, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d285d485-4a38-4552-ab6a-0438ddbe04a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-03T12:28:32.443272Z",
     "iopub.status.busy": "2022-10-03T12:28:32.442328Z",
     "iopub.status.idle": "2022-10-03T12:28:58.937047Z",
     "shell.execute_reply": "2022-10-03T12:28:58.935985Z",
     "shell.execute_reply.started": "2022-10-03T12:28:32.443190Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPS: 0\n",
      "SPS: 669\n",
      "SPS: 687\n",
      "SPS: 684\n",
      "SPS: 680\n",
      "SPS: 686\n",
      "SPS: 687\n",
      "SPS: 686\n",
      "SPS: 685\n",
      "SPS: 684\n",
      "SPS: 684\n",
      "SPS: 684\n",
      "SPS: 684\n",
      "SPS: 684\n",
      "SPS: 684\n",
      "SPS: 684\n",
      "SPS: 682\n",
      "SPS: 681\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [12], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m global_step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(args\u001b[38;5;241m.\u001b[39mtotal_timesteps):\n\u001b[0;32m----> 7\u001b[0m     next_obs, loss, metrics \u001b[38;5;241m=\u001b[39m train_step(obs, global_step)\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# if 'episodic_return' in metrics.keys():\u001b[39;00m\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;66;03m# pbar.set_postfix(episodic_return=metrics['episodic_return'])\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     obs \u001b[38;5;241m=\u001b[39m next_obs\n",
      "Cell \u001b[0;32mIn [10], line 11\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(obs, global_step)\u001b[0m\n\u001b[1;32m      8\u001b[0m     actions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(q_values, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# TRY NOT TO MODIFY: execute the game and log data.\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m next_obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[43menvs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     14\u001b[0m metrics \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/gym/vector/vector_env.py:94\u001b[0m, in \u001b[0;36mVectorEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Take an action for each sub-environments.\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \n\u001b[1;32m     73\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;124;03m    A list of auxiliary diagnostic information dicts from sub-environments.\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[0;32m---> 94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/gym/vector/sync_vector_env.py:83\u001b[0m, in \u001b[0;36mSyncVectorEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     81\u001b[0m observations, infos \u001b[38;5;241m=\u001b[39m [], []\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (env, action) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_actions)):\n\u001b[0;32m---> 83\u001b[0m     observation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rewards[i], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dones[i], info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dones[i]:\n\u001b[1;32m     85\u001b[0m         observation \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/gym/wrappers/frame_stack.py:115\u001b[0m, in \u001b[0;36mFrameStack.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m--> 115\u001b[0m     observation, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframes\u001b[38;5;241m.\u001b[39mappend(observation)\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation(), reward, done, info\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/gym/core.py:323\u001b[0m, in \u001b[0;36mObservationWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m--> 323\u001b[0m     observation, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation(observation), reward, done, info\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/gym/core.py:323\u001b[0m, in \u001b[0;36mObservationWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m--> 323\u001b[0m     observation, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation(observation), reward, done, info\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/gym/core.py:336\u001b[0m, in \u001b[0;36mRewardWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m--> 336\u001b[0m     observation, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m observation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreward(reward), done, info\n",
      "File \u001b[0;32m/app/notebooks/atari.py:120\u001b[0m, in \u001b[0;36mFireResetEnv.step\u001b[0;34m(self, ac)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, ac):\n\u001b[0;32m--> 120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mac\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/app/notebooks/atari.py:75\u001b[0m, in \u001b[0;36mEpisodicLifeEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m---> 75\u001b[0m     obs, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwas_real_done \u001b[38;5;241m=\u001b[39m done\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;66;03m# check current lives, make loss of life terminal,\u001b[39;00m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;66;03m# then update lives to handle bonus lives\u001b[39;00m\n",
      "File \u001b[0;32m/app/notebooks/atari.py:49\u001b[0m, in \u001b[0;36mMaxAndSkipEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     47\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_skip):\n\u001b[0;32m---> 49\u001b[0m     obs, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_skip \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obs_buffer[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m obs\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_skip \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obs_buffer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m obs\n",
      "File \u001b[0;32m/app/notebooks/atari.py:33\u001b[0m, in \u001b[0;36mNoopResetEnv.step\u001b[0;34m(self, ac)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, ac):\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mac\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/gym/wrappers/record_episode_statistics.py:26\u001b[0m, in \u001b[0;36mRecordEpisodeStatistics.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m---> 26\u001b[0m     observations, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mRecordEpisodeStatistics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43maction\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisode_returns \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m rewards\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisode_lengths \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/gym/core.py:289\u001b[0m, in \u001b[0;36mWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m--> 289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/gym/wrappers/time_limit.py:18\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m     16\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 18\u001b[0m     observation, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/gym/envs/atari/environment.py:238\u001b[0m, in \u001b[0;36mAtariEnv.step\u001b[0;34m(self, action_ind)\u001b[0m\n\u001b[1;32m    236\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(frameskip):\n\u001b[0;32m--> 238\u001b[0m     reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43male\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_obs(), reward, terminal, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_info()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm_notebook\n",
    "\n",
    "obs = envs.reset()\n",
    "# pbar = tqdm_notebook(range(args.total_timesteps), desc=\"DiscreteDQN training\", unit=\"step\")\n",
    "start_time = time.time()\n",
    "for global_step in range(args.total_timesteps):\n",
    "    next_obs, loss, metrics = train_step(obs, global_step)\n",
    "    # if 'episodic_return' in metrics.keys():\n",
    "        # pbar.set_postfix(episodic_return=metrics['episodic_return'])\n",
    "    obs = next_obs\n",
    "    if global_step % 1000 == 0:\n",
    "        print(\"SPS:\", int(global_step / (time.time() - start_time)))\n",
    "\n",
    "envs.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2989579d-fc7f-4650-8fdf-1be7effe52d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-03T12:29:06.916335Z",
     "iopub.status.busy": "2022-10-03T12:29:06.915416Z",
     "iopub.status.idle": "2022-10-03T12:29:06.924683Z",
     "shell.execute_reply": "2022-10-03T12:29:06.924090Z",
     "shell.execute_reply.started": "2022-10-03T12:29:06.916254Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
